{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ijson\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossier_nf = \"/home/marinedjaffardjy/Documents/Code/Similarite_process/json/ngram_nf_tools\"\n",
    "dossier_snk = \"/home/marinedjaffardjy/Documents/Code/Similarite_process/json/levenshtein_snk_tools\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/marinedjaffardjy/Documents/Code/Similarite_process/json/levenshtein_nf_tools/new/levenshtein_nf_tools_50.json\n",
      "/home/marinedjaffardjy/Documents/Code/Similarite_process/json/levenshtein_nf_tools/new/levenshtein_nf_tools_100.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7946ac1626eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         new_rec={\"rule1\":record['rule1'],\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0;34m\"rule2\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rule2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \"levenshtein\":float(record[\"levenshtein\"])}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "large_nf=\"/home/marinedjaffardjy/Documents/Code/Similarite_process/json/levenshtein_nf_tools/old/levenshtein_nf_tools_3800.json\"\n",
    "\n",
    "with open(large_nf, \"rb\") as f:\n",
    "    i=0\n",
    "    j=9651\n",
    "    x=0\n",
    "    x_old=[0]\n",
    "    new_file=[]\n",
    "    for record in ijson.items(f, \"item\"):\n",
    "        \n",
    "        \n",
    "        if i==j:\n",
    "            j=j-1\n",
    "            x+=1\n",
    "            i=0\n",
    "            if x%50==0 and x not in x_old:\n",
    "                filename=dossier_nf+\"/new/levenshtein_nf_tools_\"+str(x)+'.json'\n",
    "                with open(filename,'w') as file:\n",
    "                    json.dump(new_file,file)\n",
    "                print(filename)\n",
    "                new_file=[]\n",
    "                x_old.append(x)\n",
    "          \n",
    "        \n",
    "        new_rec={\"rule1\":record['rule1'],\n",
    "                \"rule2\": record[\"rule2\"],\n",
    "                \"levenshtein\":float(record[\"levenshtein\"])}\n",
    "        new_file.append(new_rec)  \n",
    "                \n",
    "        i+=1\n",
    "\n",
    "print(x)\n",
    "filename=dossier_nf+'/new/levenshtein_nf_tools_'+str(3800)+'.json'\n",
    "with open(filename,'w') as file:\n",
    "    json.dump(new_file,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119.0"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5950/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_csv=\"/home/marinedjaffardjy/Documents/Code/Similarite_process/csv/simil_matrix_ngram_nf_tools\"\n",
    "path_matrix=\"/home/marinedjaffardjy/Documents/Code/Similarite_process/json/matrix_nf_tools\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_csv=\"/home/marinedjaffardjy/Documents/Code/Similarite_process/csv/simil_matrix_nf_tools\"\n",
    "\n",
    "#create filenames\n",
    "filenames=[]\n",
    "path_model=\"/home/marinedjaffardjy/Documents/Code/Similarite_process/json/ngram_nf_tools/ngram_3_nf_tools_\"\n",
    "for i in range(1,110):\n",
    "    filenames.append(path_model+str(i*50)+\".json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/marinedjaffardjy/Documents/Code/Similarite_process/json/levenshtein_nf_tools/levenshtein_nf_tools_5950.json'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "5000\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n"
     ]
    }
   ],
   "source": [
    "#make and save matrix score\n",
    "path_matrix=\"/home/marinedjaffardjy/Documents/Code/Similarite_process/json/matrix_nf_tools\"\n",
    "j=9651\n",
    "z=0\n",
    "x=0\n",
    "for file in filenames:\n",
    "    new_matrix=[]\n",
    "    x+=50\n",
    "    with open(file) as f:\n",
    "        new_lines=json.load(f)\n",
    "    for i in range(0,50):\n",
    "        new_scores=[el[\"ngram\"] for el in new_lines[0:j-z]]\n",
    "        scores_el = [1]+new_scores\n",
    "        zeros = np.zeros(z)\n",
    "        z+=1\n",
    "        new_matrix.append(list(zeros)+scores_el)\n",
    "        new_lines=new_lines[j-z:]\n",
    "    print(x)    \n",
    "    matrix_filename= path_matrix+\"/matrix_nf_\"+str(x)+\".json\"\n",
    "    \n",
    "    with open(matrix_filename,\"w\") as file_json:\n",
    "        json.dump(new_matrix,file_json)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_filenames=[]\n",
    "for x in range(1,110):\n",
    "    matrix_filenames.append(path_matrix+\"/matrix_nf_\"+str(x*50)+\".json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_scores=[]\n",
    "for name in matrix_filenames:\n",
    "    with open(name) as f_nm:\n",
    "        new_m=json.load(f_nm)\n",
    "    matrix_scores+=new_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 17.8 s, total: 1min 17s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(\"/home/marinedjaffardjy/Documents/Code/Similarite_process/json/matrix_ngram_nf_tools.json\",\"w\") as f:\n",
    "    json.dump(matrix_scores,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(\"/home/marinedjaffardjy/Documents/Code/Similarite_process/json/matrix_nf_tools.json\",\"w\") as f:\n",
    "    json.dump(matrix_scores,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importing_json_files(file_wf):\n",
    "    f_wf = open(file_wf) #informations for nf\n",
    "    # returns JSON object as\n",
    "    # a dictionary\n",
    "    wf = json.load(f_wf)\n",
    "    f_wf.close\n",
    "    return wf\n",
    "\n",
    "#importing the wf and auth dict (github info)\n",
    "dict_nf = importing_json_files('json/wf_new_crawl_nextflow.json')\n",
    "auth_nf = importing_json_files('json/author_clem_nf.json')\n",
    "nf_rules=importing_json_files(\"/home/marinedjaffardjy/Documents/Code/Similarite_process/json/nf_proc_tool.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "740"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find indexes in wf nfcore\n",
    "nf_core_proc=[]\n",
    "i=0\n",
    "for el in nf_rules:\n",
    "    if el[\"owner\"]==\"nf-core\":\n",
    "        nf_core_proc.append(i)\n",
    "    i+=1\n",
    "len(nf_core_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping_simple(matrice_sim,obj_liste,treshold=0.85,ignore_nf=False):\n",
    "    to_ignore=[]\n",
    "    if ignore_nf==True:\n",
    "        to_ignore=nf_core_proc #proc in wf nfcore indexes\n",
    "    groups=[]\n",
    "    i=0\n",
    "    for i in range(0,len(matrice_sim)):\n",
    "        if i not in to_ignore:\n",
    "            new_group=[i]\n",
    "            for j in range(0,len(matrice_sim[i])):\n",
    "                if(matrice_sim[i][j]>treshold):\n",
    "                    new_group.append(j)\n",
    "            groups.append(list(set(new_group)))\n",
    "            to_ignore+=new_group\n",
    "            \n",
    "    #translate the groups into snk or nf rules\n",
    "    simil_obj=[]\n",
    "    for group in groups:\n",
    "        group_obj = []\n",
    "        for el in group:\n",
    "            group_obj.append(obj_liste[el])\n",
    "        simil_obj.append(group_obj)\n",
    "    return simil_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.5 s, sys: 20.6 ms, total: 3.52 s\n",
      "Wall time: 3.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "group_nf = grouping_simple(matrix_scores,nf_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/marinedjaffardjy/Documents/Code/Similarite_process/json/simil_groups_ngram_nf_tools.json\",\"w\") as f:\n",
    "    json.dump(group_nf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_to_df_nf_json(groups_list, dict_wf): \n",
    "    groups_sim_df = []\n",
    "    for group in groups_list :\n",
    "        list_own = []\n",
    "        list_wf = []\n",
    "        list_contrib = []\n",
    "        for el in group :\n",
    "            list_own.append(el['owner'])\n",
    "            list_wf.append(el['wf_orig'])\n",
    "            name_wf = el['owner']+'/'+el['wf_orig']\n",
    "            if (name_wf in dict_wf.keys()):\n",
    "                for contr in dict_wf[name_wf]['contributors']:\n",
    "                    list_contrib.append(contr)\n",
    "            elif (name_wf+\".nf\" in dict_wf.keys()):\n",
    "                for contr in dict_wf[name_wf+\".nf\"]['contributors']:\n",
    "                    list_contrib.append(contr)\n",
    "        list_own = list(set(list_own))\n",
    "        list_wf = list(set(list_wf))\n",
    "        list_contrib = list(set(list_contrib))\n",
    "        #print(group[0]['tools'])\n",
    "        groups_sim_df.append({'nb_reuse' : len(group),\n",
    "                                              'tools' : group[0]['tools'],\n",
    "                                             'nb_own' : len(set(list_own)),\n",
    "                                             'list_own' : list_own ,\n",
    "                                             'nb_wf' : len(set(list_wf)),\n",
    "                                             'list_wf' : list(set(list_wf)),\n",
    "                                             'list_contrib' : list(set(list_contrib)),\n",
    "                                             'nb_contrib' :len(set(list_contrib))})\n",
    "    #print(groups_sim_df)\n",
    "    return groups_sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_to_df_nf(groups_list, dict_wf): \n",
    "    groups_sim_df = pd.DataFrame(columns = ['nb_reuse', 'tools','nb_own', 'list_own', 'nb_contrib','list_contrib','nb_wf','list_wf'])\n",
    "    for group in groups_list :\n",
    "        list_own = []\n",
    "        list_wf = []\n",
    "        list_contrib = []\n",
    "        for el in group :\n",
    "            list_own.append(el['owner'])\n",
    "            list_wf.append(el['wf_orig'])\n",
    "            name_wf = el['owner']+'/'+el['wf_orig']\n",
    "            if (name_wf in dict_wf.keys()):\n",
    "                for contr in dict_wf[name_wf]['contributors']:\n",
    "                    list_contrib.append(contr)\n",
    "            elif (name_wf+\".nf\" in dict_wf.keys()):\n",
    "                for contr in dict_wf[name_wf+\".nf\"]['contributors']:\n",
    "                    list_contrib.append(contr)\n",
    "        list_own = list(set(list_own))\n",
    "        list_wf = list(set(list_wf))\n",
    "        list_contrib = list(set(list_contrib))\n",
    "        #print(group[0]['tools'])\n",
    "        groups_sim_df = groups_sim_df.append({'nb_reuse' : len(group),\n",
    "                                              'tools' : group[0]['tools'],\n",
    "                                             'nb_own' : len(set(list_own)),\n",
    "                                             'list_own' : list_own ,\n",
    "                                             'nb_wf' : len(set(list_wf)),\n",
    "                                             'list_wf' : list(set(list_wf)),\n",
    "                                             'list_contrib' : list(set(list_contrib)),\n",
    "                                             'nb_contrib' :len(set(list_contrib))}, ignore_index = True)\n",
    "    #print(groups_sim_df)\n",
    "    return groups_sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df_nf=sim_to_df_nf(group_nf,dict_nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df_nf.to_csv('csv/df_groups_ngram_tools_nf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/marinedjaffardjy/Documents/Code/Similarite_process/json/simil_groups_ngram_nf_tools.json\") as f:\n",
    "    group_nf=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df_nf2=sim_to_df_nf_json(group_nf,dict_nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/marinedjaffardjy/Documents/Code/Similarite_process/json/simil_groups_ngram_nf_tools_df.json\",\"w\") as f:\n",
    "    json.dump(sim_df_nf2,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df_nf=pd.read_csv('csv/df_groups_lev_tools_nf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the same df and groups without nfcore wf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the indexes of groups that don't contain nf-core proc\n",
    "i=0\n",
    "groups_to_ignore=[]\n",
    "for list_own in sim_df_nf[\"list_own\"]:\n",
    "    list_own_df=list_own.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "    list_own_df=list_own_df.split(\",\")\n",
    "    for own in list_own_df:\n",
    "        #print(own)\n",
    "        if \"nf-core\" in own:\n",
    "            groups_to_ignore.append(i)\n",
    "    i+=1\n",
    "\n",
    "#get the data without nfcore df[df.index.isin([1,3])]\n",
    "sim_df_nf_wo_nfc = sim_df_nf[sim_df_nf.index.isin(groups_to_ignore)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_reuse</th>\n",
       "      <th>tools</th>\n",
       "      <th>nb_own</th>\n",
       "      <th>list_own</th>\n",
       "      <th>nb_contrib</th>\n",
       "      <th>list_contrib</th>\n",
       "      <th>nb_wf</th>\n",
       "      <th>list_wf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>[Minimap2]</td>\n",
       "      <td>3</td>\n",
       "      <td>[ksumngs, ABMicroBioinf, nf-core]</td>\n",
       "      <td>106</td>\n",
       "      <td>[ewels, oschwengers, Darcy220606, Emiller88, n...</td>\n",
       "      <td>3</td>\n",
       "      <td>[nf-modules, magph, modules]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>[MetaPhlAn, Bowtie]</td>\n",
       "      <td>4</td>\n",
       "      <td>[jianhong, ABMicroBioinf, nf-core, xiaoli-dong]</td>\n",
       "      <td>106</td>\n",
       "      <td>[ewels, oschwengers, Darcy220606, Emiller88, n...</td>\n",
       "      <td>3</td>\n",
       "      <td>[magph, modules, shotgun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[Bracken]</td>\n",
       "      <td>3</td>\n",
       "      <td>[xmuyulab, ABMicroBioinf, xiaoli-dong]</td>\n",
       "      <td>3</td>\n",
       "      <td>[xiaoli-dong, lmyiing, huhehaotecrystal]</td>\n",
       "      <td>3</td>\n",
       "      <td>[pathogen, magph, scRNAseq_pipelines]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>[Filtlong]</td>\n",
       "      <td>1</td>\n",
       "      <td>[ABMicroBioinf]</td>\n",
       "      <td>1</td>\n",
       "      <td>[xiaoli-dong]</td>\n",
       "      <td>2</td>\n",
       "      <td>[pathogen, magph]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>[SAMtools]</td>\n",
       "      <td>5</td>\n",
       "      <td>[sguizard, xiaoli-dong, xmuyulab, ABMicroBioin...</td>\n",
       "      <td>8</td>\n",
       "      <td>[jmarvil, huhehaotecrystal, peterwharrison, lm...</td>\n",
       "      <td>5</td>\n",
       "      <td>[emucat_workflows, scRNAseq_pipelines, pathoge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  nb_reuse                tools nb_own  \\\n",
       "0        3           [Minimap2]      3   \n",
       "1        4  [MetaPhlAn, Bowtie]      4   \n",
       "2        4            [Bracken]      3   \n",
       "3        2           [Filtlong]      1   \n",
       "4       12           [SAMtools]      5   \n",
       "\n",
       "                                            list_own nb_contrib  \\\n",
       "0                  [ksumngs, ABMicroBioinf, nf-core]        106   \n",
       "1    [jianhong, ABMicroBioinf, nf-core, xiaoli-dong]        106   \n",
       "2             [xmuyulab, ABMicroBioinf, xiaoli-dong]          3   \n",
       "3                                    [ABMicroBioinf]          1   \n",
       "4  [sguizard, xiaoli-dong, xmuyulab, ABMicroBioin...          8   \n",
       "\n",
       "                                        list_contrib nb_wf  \\\n",
       "0  [ewels, oschwengers, Darcy220606, Emiller88, n...     3   \n",
       "1  [ewels, oschwengers, Darcy220606, Emiller88, n...     3   \n",
       "2           [xiaoli-dong, lmyiing, huhehaotecrystal]     3   \n",
       "3                                      [xiaoli-dong]     2   \n",
       "4  [jmarvil, huhehaotecrystal, peterwharrison, lm...     5   \n",
       "\n",
       "                                             list_wf  \n",
       "0                       [nf-modules, magph, modules]  \n",
       "1                          [magph, modules, shotgun]  \n",
       "2              [pathogen, magph, scRNAseq_pipelines]  \n",
       "3                                  [pathogen, magph]  \n",
       "4  [emucat_workflows, scRNAseq_pipelines, pathoge...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_df_nf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(sim_df_nf[\"nb_reuse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df_nf_wo_nfc.to_csv('csv/df_groups_lev_tools_nf_wo_nfc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nb_reuse': 66, 'tools': ['Salmon'], 'nb_own': 56, 'list_own': ['IARCbioinfo', 'lengfei5', 'mbosio85', 'chelauk', 'brentp', 'vibbits', 'nf-core', 'h3abionet', 'BrianLohman', 'oisinmccaffrey', 'luslab', 'abhi18av', 'SimonDMurray', 'bigbio', 'ksumngs', 'jvierstra', 'wbazant', 'pblaney', 'hoelzer', 'MGordon09', 'elyadlezmi', 'ralsallaq', 'DaneVass', 'BarryDigby', 'stevekm', 'bioatlas', 'markgene', 'grst', 'gates-mri-bioinformatics', 'caspargross', 'maxibor', 'cometsong', 'leipzig', 'nibscbioinformatics', 'jtmccr1', 'noamteyssier', 'bcgsc', 'GMS6804-master', 'czbiohub', 'zamanianlab', 'GFrosi', 'junyu-boston', 'tdelhomme', 'NorwegianVeterinaryInstitute', 'Monia234', 'FredHutch', 'nilesh-tawari', 'stjacqrm', 'esrice', 'sarseq', 'marcodelapierre', 'MDegener', 'tamara-hodgetts', 'eQTL-Catalogue', 'gongyh', 'BrianReevz'], 'nb_wf': 62, 'list_wf': ['ATAC_Seq_nxf', 'ngs_variant_calling', 'clipseq.nextflow', 'sarseq2', 'nf-core-platypus', 'ampliflow', 'mgp1000', 'm6a', 'circ', 'nf-extractcoding', 'bft-nf', 'reference-database', 'sc2-ngs-analysis', 'Core_RNAseq-nf', 'illumina-nf', 'workflows', 'nf-core-umi_preprocessing', 'star-rsem-scrnaseq-nf', 'cutnrun', 'dvc-pipelines', 'TMBur', 'nextflow-pipeline-demo', 'nf-modules', 'RNA2CM', 'Imputation-nf', 'blast-pipeline', 'nf-sicilian', 'nf-core-modules', 'rnatoy', 'nf-core-gatkcohortcall', 'chipimputation_evaluate_chips', 'Talos', 'plasmIDent', 'genimpute', 'chipseq-nextflow', 'nf-atac-seq', 'BARtab', 'DNA-pipeline', 'nf-tracer', 'nextflow_pipelines', 'Nextflow-BEDtoBAM-IndexBAM', 'fastqc-nf', 'nf-core-buggybarcodes', 'nf-genotyping', 'viralrecon', 'gatk4-GenotypeGVCFs-IARC', 'nextflow_grid_search', 'dicerna-rnaseq', 'bd_rhapsody_velocyto', 'berrywood', 'NEMO', 'humann-nf', 'vcf_ancestry-nf', 'nf_visium_kallisto', 'CORALE', 'RNAseq-pipeline', 'nf-workflows', 'nf-core-scgs', 'GoogleCloudLifeSciSeq', 'modules', 'metaGx_nf', 'rare-disease-wf'], 'list_contrib': ['pcantalupo', 'ewels', 'kauralasoo', 'BrianLohman', 'jvierstra', 'kerimoff', 'pblaney', 'marchoeppner', 'DiegoBrambilla', 'na399', 'stevin-wilson', 'MGordon09', 'FriederikeHanssen', 'ralsallaq', 'stevekm', 'maxulysse', 'rfenouil', 'grst', 'lpantano', 'orionzhou', 'veeravalli', 'caspargross', 'kviljoen', 'silviamorins', 'sureshhewabi', 'leipzig', 'ppericard', 'winni2k', 'apeltzer', 'ImaneLboukili', 'mashehu', 'jun-wan', 'd4straub', 'husensofteng', 'amayer21', 'ralf-tambets', 'sofiahaglund', 'mvanins', 'tamara-hodgetts', 'liameabbott', 'jordwil', 'valleem', 'gongyh', 'vsoch', 'ypriverol', 'mamanambiya', 'mzamanian', 'jburos', 'jcurado-flomics', 'wbazant', 'jgriss', 'robsyme', 'nf-core-bot', 'RichardCorbett', 'tmuylder', 'mfoll', 'saramonzon', 'skrakau', 'ggabernet', 'JoseEspinosa', 'aanil', 'paulklemm', 'matrulda', 'ErikaKvalem', 'Galithil', 'alneberg', 'chuan-wang', 'chelauk', 'brentp', 'heuermh', 'arontommi', 'oisinmccaffrey', 'abhi18av', 'olgabot', 'marc-jones', 'BABS-STP1', 'pditommaso', 'aurelieGabriel', 'CharlotteAnne', 'Thomieh73', 'jemten', 'svarona', 'maxibor', 'amchakra', 'cometsong', 'jtmccr1', 'noamteyssier', 'Lipinski-B', 'KevinMenden', 'Hammarn', 'erikrikarddaniel', 'tdelhomme', 'dlemas', 'colindaven', 'ameintjes', 'BrianReevz', 'kcleal', 'lengfei5', 'mbosio85', 'chenthorn', 'tiagochst', 'kaitlinchaung', 'drejom', 'vezzi', 'SimonDMurray', 'drpatelh', 'nilesh-tawari-elanco', 'hoelzer', 'sven1103', 'elyadlezmi', 'senthil10', 'Rotholandus', 'DaneVass', 'mzager', 'BarryDigby', 'antunderwood', 'ktrns', 'markgene', 'v-clarissa', 'samlhao', 'mwalzer', 'MillironX', 'pranathivemuri', 'GFrosi', 'charlotte-west', 'stjacqrm', 'MiguelJulia', 'esrice', 'zxl124', 'rsuchecki', 'sarseq', 'alexthiery', 'lescai', 'marcodelapierre', 'MDegener', 'chris-cheshire', 'nalcala'], 'nb_contrib': 137}\n"
     ]
    }
   ],
   "source": [
    "for el in sim_df_nf2:\n",
    "    if(el[\"nb_reuse\"]==66):\n",
    "        print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
