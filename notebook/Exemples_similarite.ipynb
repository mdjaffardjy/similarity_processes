{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import jellyfish\n",
    "import time\n",
    "import ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read similarity scores for pairs of processes from the matrix\n",
    "\n",
    "We want to look up the code of processes, in snakemake and in nextflow, as well as their levenshtein and 3-gram similarity scores. \n",
    "We want to look up pairs of similar and different sizes, small and large processes, processes that are very reused vs processes that are similar to very little other processes, processes using the same tool and not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import similarity matrices\n",
    "path_matrix_nf_lev=\"/home/marinedjaffardjy/Documents/Code/Similarite_process/json/matrix_nf_levenshtein\"\n",
    "path_matrix_snk_lev=\"/home/marinedjaffardjy/Documents/Code/Similarite_process/json/matrix_snk_levenshtein\"\n",
    "path_matrix_nf_ngram=\"/home/marinedjaffardjy/Documents/Code/Similarite_process/json/matrix_nf_ngram\"\n",
    "path_matrix_snk_ngram=\"/home/marinedjaffardjy/Documents/Code/Similarite_process/json/matrix_snk_ngram\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importing_json_files(file_wf):\n",
    "    f_wf = open(file_wf) #informations for nf\n",
    "    # returns JSON object as\n",
    "    # a dictionary\n",
    "    wf = json.load(f_wf)\n",
    "    f_wf.close\n",
    "    return wf\n",
    "\n",
    "#importing the wf and auth dict (github info)\n",
    "dict_nf = importing_json_files('../json/wf_new_crawl_nextflow.json')\n",
    "auth_nf = importing_json_files('../json/author_clem_nf.json')\n",
    "dict_snk = importing_json_files('../json/wf_crawl_snakemake.json')\n",
    "auth_snk = importing_json_files('../json/author_clem_snk.json')\n",
    "\n",
    "#import processes dictionnaries\n",
    "#snakemake rules\n",
    "with open('/home/marinedjaffardjy/Documents/Code/Investigating_reuse/json/snk_rule_info_tool.json') as f:\n",
    "    snk_proc = json.load(f)\n",
    "#nf proc\n",
    "with open('/home/marinedjaffardjy/Documents/Code/Similarite_process/json/nf_proc_tool.json') as f:\n",
    "    nf_proc = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "examinons diff√©rents seuils :\n",
    "- 0.80\n",
    "- 0.85\n",
    "- 0.90\n",
    "- 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_files(path_matrix,nb_proc):\n",
    "    mat_nf_ngram=[]\n",
    "    i=50\n",
    "    while i <=(nb_proc-1):\n",
    "        matrix_filename= path_matrix+\"/matrix_\"+str(i)+\".json\"\n",
    "        mat_nf_ngram.append(matrix_filename)\n",
    "        i+=50\n",
    "    if((nb_proc-1)%50!=0):\n",
    "        matrix_filename= path_matrix+\"/matrix_\"+str(nb_proc-1)+\".json\"\n",
    "        mat_nf_ngram.append(matrix_filename) \n",
    "    return mat_nf_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_nf_lev = get_matrix_files(path_matrix_nf_lev,9652)\n",
    "mat_nf_ngram = get_matrix_files(path_matrix_nf_ngram,9652)\n",
    "mat_snk_lev = get_matrix_files(path_matrix_snk_lev,5001)\n",
    "mat_snk_ngram = get_matrix_files(path_matrix_snk_ngram,5001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs_brackets(mat_nf_lev):\n",
    "    seuils = [0.80,0.85,0.90,0.95]\n",
    "    i=0\n",
    "    paires3=[]\n",
    "    paires2=[]\n",
    "    paires1=[]\n",
    "    paires0=[]\n",
    "    for file in mat_nf_lev:\n",
    "        with open(file) as f:\n",
    "            new_lines=json.load(f)\n",
    "        for line in new_lines:\n",
    "            for j in range(0,len(line)):\n",
    "                if(i!=j):\n",
    "                    if(line[j]>seuils[3]):\n",
    "                        paires3.append([i,j])\n",
    "                    elif(line[j]>seuils[2]):\n",
    "                        paires2.append([i,j])\n",
    "                    elif(line[j]>seuils[1]):\n",
    "                        paires1.append([i,j])\n",
    "                    elif(line[j]>seuils[0]):\n",
    "                        paires0.append([i,j])\n",
    "            i+=1\n",
    "    return [paires0,paires1,paires2,paires3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.4 s, sys: 1.44 s, total: 49.9 s\n",
      "Wall time: 52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "paires_lev_nf = get_pairs_brackets(mat_nf_lev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.3 s, sys: 372 ms, total: 12.6 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "paires_lev_snk = get_pairs_brackets(mat_snk_lev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find pairs same tools and diff tools\n",
    "def find_diff_tools(tools_nf, pairs0):\n",
    "    same_tools = []\n",
    "    diff_tools = []\n",
    "    for pair in pairs0:\n",
    "        if(tools_nf[pair[0]]==tools_nf[pair[1]]):\n",
    "            same_tools.append(pair)\n",
    "        else:\n",
    "            diff_tools.append(pair)\n",
    "    return same_tools,diff_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_tools_nf=[]\n",
    "diff_tools_nf=[]\n",
    "tools_nf=[el[\"tools\"] for el in nf_proc]\n",
    "for i in range(0,4):\n",
    "    same_new,diff_new=find_diff_tools(tools_nf,paires_lev_nf[i])\n",
    "    same_tools_nf.append(same_new)\n",
    "    diff_tools_nf.append(diff_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seuils 0.8\n",
      "811\n",
      "7004\n",
      "seuils 0.85\n",
      "483\n",
      "4168\n",
      "seuils 0.9\n",
      "368\n",
      "4012\n",
      "seuils 0.95\n",
      "931\n",
      "9441\n"
     ]
    }
   ],
   "source": [
    "seuils = [0.80,0.85,0.90,0.95]\n",
    "for i in range(0,4):\n",
    "    print(\"seuils \"+str(seuils[i]))\n",
    "    print(len(same_tools_nf[i]))\n",
    "    print(len(diff_tools_nf[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_codes(pair,nf_proc,mat_files_lev, mat_files_ngram):\n",
    "    \n",
    "    code1 = nf_proc[pair[0]][\"code\"]\n",
    "    code2 = nf_proc[pair[1]][\"code\"]\n",
    "    file_nb = int(pair[0]/50)\n",
    "    with open(mat_files_lev[file_nb]) as f:\n",
    "        mat_lev = json.load(f)\n",
    "    with open(mat_files_ngram[file_nb]) as f:\n",
    "        mat_ngram = json.load(f)\n",
    "        \n",
    "    levenshtein = mat_lev[pair[0]%50][pair[1]]\n",
    "    score_ngram = mat_ngram[pair[0]%50][pair[1]]\n",
    "    \n",
    "    print(f\"LEVENSHTEIN : {levenshtein} \\nNGRAM : {score_ngram} \\n\")\n",
    "    print(\"PROCESS 1\")\n",
    "    print(code1)\n",
    "    print(\"PROCESS 2\")\n",
    "    print(code2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at some examples of processes that have similar and different tools, for the same score brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEVENSHTEIN : 0.8238747553816047 \n",
      "NGRAM : 0.7724498692240628 \n",
      "\n",
      "PROCESS 1\n",
      "process BEDTOOLS_MASKFASTA {\n",
      "    tag \"$meta.id\"\n",
      "    label 'process_medium'\n",
      "\n",
      "    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n",
      "    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n",
      "        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n",
      "        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n",
      "\n",
      "    input:\n",
      "    tuple val(meta), path(bed)\n",
      "    path  fasta\n",
      "\n",
      "    output:\n",
      "    tuple val(meta), path(\"*.fa\"), emit: fasta\n",
      "    path \"versions.yml\"          , emit: versions\n",
      "\n",
      "    when:\n",
      "    task.ext.when == null || task.ext.when\n",
      "\n",
      "    script:\n",
      "    def args = task.ext.args ?: ''\n",
      "    def prefix = task.ext.prefix ?: \"${meta.id}\"\n",
      "    \"\"\"\n",
      "    bedtools \\\\\n",
      "        maskfasta \\\\\n",
      "        $args \\\\\n",
      "        -fi $fasta \\\\\n",
      "        -bed $bed \\\\\n",
      "        -fo ${prefix}.fa\n",
      "    cat <<-END_VERSIONS > versions.yml\n",
      "    \"${task.process}\":\n",
      "        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n",
      "    END_VERSIONS\n",
      "    \"\"\"\n",
      "}\n",
      "PROCESS 2\n",
      "process BEDTOOLS_INTERSECT {\n",
      "    tag \"$meta.id\"\n",
      "    label 'process_medium'\n",
      "\n",
      "    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n",
      "    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n",
      "        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n",
      "        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n",
      "\n",
      "    input:\n",
      "    tuple val(meta), path(intervals1), path(intervals2)\n",
      "    val extension\n",
      "\n",
      "    output:\n",
      "    tuple val(meta), path(\"*.${extension}\"), emit: intersect\n",
      "    path  \"versions.yml\"                   , emit: versions\n",
      "\n",
      "    script:\n",
      "    def args = task.ext.args ?: ''\n",
      "    def prefix = task.ext.prefix ?: \"${meta.id}\"\n",
      "    \"\"\"\n",
      "    bedtools \\\\\n",
      "        intersect \\\\\n",
      "        -a $intervals1 \\\\\n",
      "        -b $intervals2 \\\\\n",
      "        $args \\\\\n",
      "        > ${prefix}.${extension}\n",
      "\n",
      "    cat <<-END_VERSIONS > versions.yml\n",
      "    \"${task.process}\":\n",
      "        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n",
      "    END_VERSIONS\n",
      "    \"\"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "display_codes(same_tools_nf[0][64],nf_proc, mat_nf_lev,mat_nf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEVENSHTEIN : 0.8043117744610282 \n",
      "NGRAM : 0.8252351097178683 \n",
      "\n",
      "PROCESS 1\n",
      "\n",
      "process SAMTOOLS_SORT {\n",
      "    tag \"$meta.id\"\n",
      "    label 'process_medium'\n",
      "    publishDir \"${params.outdir}\",\n",
      "        mode: params.publish_dir_mode,\n",
      "        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n",
      "\n",
      "    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n",
      "    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n",
      "        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n",
      "    } else {\n",
      "        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n",
      "    }\n",
      "\n",
      "    input:\n",
      "    tuple val(meta), path(bam)\n",
      "\n",
      "    output:\n",
      "    tuple val(meta), path(\"*.bam\"), emit: bam\n",
      "    path  \"versions.yml\"          , emit: versions\n",
      "\n",
      "    script:\n",
      "    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n",
      "    \"\"\"\n",
      "    samtools sort $options.args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n",
      "    cat <<-END_VERSIONS > versions.yml\n",
      "    ${getProcessName(task.process)}:\n",
      "        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n",
      "    END_VERSIONS\n",
      "    \"\"\"\n",
      "}\n",
      "PROCESS 2\n",
      "\n",
      "process Fastqc{\n",
      "    tag \"$sample_name\"\n",
      "    publishDir path: { params.skip_fastqc ? params.outdir : \"${params.outdir}/QC\" },\n",
      "             saveAs: { params.skip_fastqc ? null : it }, mode: 'link'\n",
      "\n",
      "    input:\n",
      "    set val(sample_name), file(reads), val(reads_single_end), val(sample_id), val(gzip), val(input), val(group) from fastqc_reads\n",
      "\n",
      "    output:\n",
      "    file \"fastqc/*\" into fastqc_results\n",
      "\n",
      "    when:\n",
      "    aligner != \"none\" && !params.skip_fastqc\n",
      "\n",
      "    shell:\n",
      "    skip_fastqc = params.skip_fastqc\n",
      "    if ( reads_single_end){\n",
      "        \"\"\"\n",
      "        mkdir fastqc\n",
      "        fastqc -o fastqc --noextract ${reads}\n",
      "        \"\"\"       \n",
      "    } else {\n",
      "        \"\"\"\n",
      "        mkdir fastqc   \n",
      "        fastqc -o fastqc --noextract ${reads[0]}\n",
      "        fastqc -o fastqc --noextract ${reads[1]}\n",
      "        \"\"\"      \n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "display_codes(diff_tools_nf[0][76],nf_proc, mat_nf_lev,mat_nf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEVENSHTEIN : 0.8922651933701657 \n",
      "NGRAM : 0.8337531486146096 \n",
      "\n",
      "PROCESS 1\n",
      "\n",
      "process fastqc {\n",
      "    tag \"$name\"\n",
      "    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n",
      "        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n",
      "\n",
      "    input:\n",
      "    set val(name), file(reads) from raw_reads_fastqc\n",
      "\n",
      "    output:\n",
      "    file \"*_fastqc.{zip,html}\" into fastqc_results\n",
      "\n",
      "    script:\n",
      "    \"\"\"\n",
      "    fastqc -q $reads\n",
      "    \"\"\"\n",
      "}\n",
      "PROCESS 2\n",
      "\n",
      "process fastqc {\n",
      "    tag \"${reads[0].baseName}\"\n",
      "    publishDir \"${params.outdir}/fastqc\", mode: 'copy',\n",
      "        saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n",
      "\n",
      "    input:\n",
      "    file reads from read_files_fastqc\n",
      "\n",
      "    output:\n",
      "    file \"*_fastqc.{zip,html}\" into fastqc_results\n",
      "\n",
      "    script:\n",
      "    \"\"\"\n",
      "    fastqc -q $reads\n",
      "    \"\"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "display_codes(same_tools_nf[1][122],nf_proc, mat_nf_lev,mat_nf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEVENSHTEIN : 0.886411889596603 \n",
      "NGRAM : 0.8386108273748724 \n",
      "\n",
      "PROCESS 1\n",
      "process SAMTOOLS_IDXSTATS {\n",
      "    tag \"$meta.id\"\n",
      "    label 'process_low'\n",
      "\n",
      "    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n",
      "    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n",
      "        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n",
      "        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n",
      "\n",
      "    input:\n",
      "    tuple val(meta), path(bam), path(bai)\n",
      "\n",
      "    output:\n",
      "    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n",
      "    path  \"versions.yml\"               , emit: versions\n",
      "\n",
      "    script:\n",
      "    def args = task.ext.args ?: ''\n",
      "    \"\"\"\n",
      "    samtools idxstats $bam > ${bam}.idxstats\n",
      "    cat <<-END_VERSIONS > versions.yml\n",
      "    \"${task.process}\":\n",
      "        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n",
      "    END_VERSIONS\n",
      "    \"\"\"\n",
      "}\n",
      "PROCESS 2\n",
      "process MULTIQC {\n",
      "    label 'process_medium'\n",
      "\n",
      "    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n",
      "    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n",
      "        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n",
      "        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n",
      "\n",
      "    input:\n",
      "    path multiqc_files\n",
      "\n",
      "    output:\n",
      "    path \"*multiqc_report.html\", emit: report\n",
      "    path \"*_data\"              , emit: data\n",
      "    path \"*_plots\"             , optional:true, emit: plots\n",
      "    path \"versions.yml\"        , emit: versions\n",
      "\n",
      "    when:\n",
      "    task.ext.when == null || task.ext.when\n",
      "\n",
      "    script:\n",
      "    def args = task.ext.args ?: ''\n",
      "    \"\"\"\n",
      "    multiqc -f $args .\n",
      "\n",
      "    cat <<-END_VERSIONS > versions.yml\n",
      "    \"${task.process}\":\n",
      "        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n",
      "    END_VERSIONS\n",
      "    \"\"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "display_codes(diff_tools_nf[1][46],nf_proc, mat_nf_lev,mat_nf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEVENSHTEIN : 0.9202453987730062 \n",
      "NGRAM : 0.8306374881065651 \n",
      "\n",
      "PROCESS 1\n",
      "process SAMTOOLS_IDXSTATS {\n",
      "    tag \"$meta.id\"\n",
      "    label 'process_low'\n",
      "\n",
      "    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n",
      "    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n",
      "        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n",
      "        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n",
      "\n",
      "    input:\n",
      "    tuple val(meta), path(bam), path(bai)\n",
      "\n",
      "    output:\n",
      "    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n",
      "    path  \"versions.yml\"               , emit: versions\n",
      "\n",
      "    when:\n",
      "    task.ext.when == null || task.ext.when\n",
      "\n",
      "    script:\n",
      "    def args = task.ext.args ?: ''\n",
      "    \"\"\"\n",
      "    samtools \\\\\n",
      "        idxstats \\\\\n",
      "        $bam \\\\\n",
      "        > ${bam}.idxstats\n",
      "\n",
      "    cat <<-END_VERSIONS > versions.yml\n",
      "    \"${task.process}\":\n",
      "        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n",
      "    END_VERSIONS\n",
      "    \"\"\"\n",
      "}\n",
      "PROCESS 2\n",
      "process SAMTOOLS_FLAGSTAT {\n",
      "    tag \"$meta.id\"\n",
      "    label 'process_low'\n",
      "\n",
      "    conda (params.enable_conda ? \"bioconda::samtools=1.14\" : null)\n",
      "    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n",
      "        'https://depot.galaxyproject.org/singularity/samtools:1.14--hb421002_0' :\n",
      "        'quay.io/biocontainers/samtools:1.14--hb421002_0' }\"\n",
      "\n",
      "    input:\n",
      "    tuple val(meta), path(bam), path(bai)\n",
      "\n",
      "    output:\n",
      "    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n",
      "    path  \"versions.yml\"               , emit: versions\n",
      "\n",
      "    when:\n",
      "    task.ext.when == null || task.ext.when\n",
      "\n",
      "    script:\n",
      "    def args = task.ext.args ?: ''\n",
      "    \"\"\"\n",
      "    samtools \\\\\n",
      "        flagstat \\\\\n",
      "        --threads ${task.cpus-1} \\\\\n",
      "        $bam \\\\\n",
      "        > ${bam}.flagstat\n",
      "\n",
      "    cat <<-END_VERSIONS > versions.yml\n",
      "    \"${task.process}\":\n",
      "        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n",
      "    END_VERSIONS\n",
      "    \"\"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "display_codes(same_tools_nf[2][248],nf_proc, mat_nf_lev,mat_nf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEVENSHTEIN : 0.9020979020979021 \n",
      "NGRAM : 0.877914951989026 \n",
      "\n",
      "PROCESS 1\n",
      "\n",
      "process get_software_versions {\n",
      "    publishDir \"${params.outdir}/pipeline_info\", mode: params.publish_dir_mode,\n",
      "        saveAs: { filename ->\n",
      "                      if (filename.indexOf(\".csv\") > 0) filename\n",
      "                      else null\n",
      "                }\n",
      "\n",
      "    output:\n",
      "    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n",
      "    file \"software_versions.csv\"\n",
      "\n",
      "    script:\n",
      "                                                                     \n",
      "    \"\"\"\n",
      "    echo $workflow.manifest.version > v_pipeline.txt\n",
      "    echo $workflow.nextflow.version > v_nextflow.txt\n",
      "    fastqc --version > v_fastqc.txt\n",
      "    multiqc --version > v_multiqc.txt\n",
      "    scrape_software_versions.py &> software_versions_mqc.yaml\n",
      "    \"\"\"\n",
      "}\n",
      "PROCESS 2\n",
      "\n",
      "process mutect2_nChunk {\n",
      "    tag \"${prefix}\"\n",
      "    publishDir \"${params.outputDir}/variants\", overwrite: true, mode: 'copy'\n",
      "    echo true\n",
      "\n",
      "    input:\n",
      "    set val(chunkLabel), val(targetChunkNum), val(comparisonID), val(tumorID), val(normalID), file(tumorBam), file(tumorBai), file(normalBam), file(normalBai), file(\"targets.bed\"), file(ref_fasta), file(ref_fai), file(ref_dict), file(dbsnp_ref_vcf), file(dbsnp_ref_vcf_idx), file(cosmic_ref_vcf), file(cosmic_ref_vcf_idx) from input_nChunk_ch\n",
      "\n",
      "    output:\n",
      "    set val(chunkLabel), val(targetChunkNum), val(comparisonID), val(tumorID), val(normalID), file(\"${output_norm_vcf}\") into variants_nChunk\n",
      "    file(\"${output_vcf}\")\n",
      "    file(\"${multiallelics_stats}\")\n",
      "    file(\"${realign_stats}\")\n",
      "\n",
      "    when: params.disable != \"true\"\n",
      "\n",
      "    script:\n",
      "                                           \n",
      "    prefix = \"${comparisonID}.${chunkLabel}.${targetChunkNum}\"\n",
      "    output_vcf = \"${prefix}.vcf\"\n",
      "    output_norm_vcf = \"${prefix}.norm.vcf\"\n",
      "    multiallelics_stats = \"${prefix}.bcftools.multiallelics.stats.txt\"\n",
      "    realign_stats = \"${prefix}.bcftools.realign.stats.txt\"\n",
      "    \"\"\"\n",
      "    gatk.sh -T MuTect2 \\\n",
      "    -dt NONE \\\n",
      "    --logging_level WARN \\\n",
      "    --standard_min_confidence_threshold_for_calling 30 \\\n",
      "    --max_alt_alleles_in_normal_count 10 \\\n",
      "    --max_alt_allele_in_normal_fraction 0.05 \\\n",
      "    --max_alt_alleles_in_normal_qscore_sum 40 \\\n",
      "    --reference_sequence \"${ref_fasta}\" \\\n",
      "    --dbsnp \"${dbsnp_ref_vcf}\" \\\n",
      "    --cosmic \"${cosmic_ref_vcf}\" \\\n",
      "    --intervals \"targets.bed\" \\\n",
      "    --interval_padding 10 \\\n",
      "    --input_file:tumor \"${tumorBam}\" \\\n",
      "    --input_file:normal \"${normalBam}\" \\\n",
      "    --out \"${output_vcf}\"\n",
      "\n",
      "    # normalize and split vcf entries\n",
      "    cat ${output_vcf} | \\\n",
      "    bcftools norm --multiallelics -both --output-type v - 2>\"${multiallelics_stats}\" | \\\n",
      "    bcftools norm --fasta-ref \"${ref_fasta}\" --output-type v - 2>\"${realign_stats}\" > \\\n",
      "    \"${output_norm_vcf}\"\n",
      "    \"\"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "display_codes(diff_tools_nf[2][176],nf_proc, mat_nf_lev,mat_nf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEVENSHTEIN : 0.9592391304347826 \n",
      "NGRAM : 0.9494268374915711 \n",
      "\n",
      "PROCESS 1\n",
      "process PICARD_MARKDUPLICATES {\n",
      "    tag \"$meta.id\"\n",
      "    label 'process_medium'\n",
      "\n",
      "    conda (params.enable_conda ? \"bioconda::picard=2.26.10\" : null)\n",
      "    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n",
      "        'https://depot.galaxyproject.org/singularity/picard:2.26.10--hdfd78af_0' :\n",
      "        'quay.io/biocontainers/picard:2.26.10--hdfd78af_0' }\"\n",
      "\n",
      "    input:\n",
      "    tuple val(meta), path(bam)\n",
      "\n",
      "    output:\n",
      "    tuple val(meta), path(\"*.bam\")        , emit: bam\n",
      "    tuple val(meta), path(\"*.bai\")        , optional:true, emit: bai\n",
      "    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n",
      "    path  \"versions.yml\"                  , emit: versions\n",
      "\n",
      "    when:\n",
      "    task.ext.when == null || task.ext.when\n",
      "\n",
      "    script:\n",
      "    def args = task.ext.args ?: ''\n",
      "    def prefix = task.ext.prefix ?: \"${meta.id}\"\n",
      "    def avail_mem = 3\n",
      "    if (!task.memory) {\n",
      "        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n",
      "    } else {\n",
      "        avail_mem = task.memory.giga\n",
      "    }\n",
      "    \"\"\"\n",
      "    picard \\\\\n",
      "        -Xmx${avail_mem}g \\\\\n",
      "        MarkDuplicates \\\\\n",
      "        $args \\\\\n",
      "        I=$bam \\\\\n",
      "        O=${prefix}.bam \\\\\n",
      "        M=${prefix}.MarkDuplicates.metrics.txt\n",
      "\n",
      "    cat <<-END_VERSIONS > versions.yml\n",
      "    \"${task.process}\":\n",
      "        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n",
      "    END_VERSIONS\n",
      "    \"\"\"\n",
      "}\n",
      "PROCESS 2\n",
      "process PICARD_MARKDUPLICATES {\n",
      "    tag \"$meta.id\"\n",
      "    label 'process_medium'\n",
      "\n",
      "    conda (params.enable_conda ? \"bioconda::picard=2.26.7\" : null)\n",
      "    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n",
      "        'https://depot.galaxyproject.org/singularity/picard:2.26.7--hdfd78af_0' :\n",
      "        'quay.io/biocontainers/picard:2.26.7--hdfd78af_0' }\"\n",
      "\n",
      "    input:\n",
      "    tuple val(meta), path(bam)\n",
      "\n",
      "    output:\n",
      "    tuple val(meta), path(\"*.bam\")        , emit: bam\n",
      "    tuple val(meta), path(\"*.bai\")        , optional:true, emit: bai\n",
      "    tuple val(meta), path(\"*.metrics.txt\"), emit: metrics\n",
      "    path  \"versions.yml\"                  , emit: versions\n",
      "\n",
      "    script:\n",
      "    def args = task.ext.args ?: ''\n",
      "    def prefix = task.ext.prefix ?: \"${meta.id}\"\n",
      "    def avail_mem = 3\n",
      "    if (!task.memory) {\n",
      "        log.info '[Picard MarkDuplicates] Available memory not known - defaulting to 3GB. Specify process memory requirements to change this.'\n",
      "    } else {\n",
      "        avail_mem = task.memory.giga\n",
      "    }\n",
      "    \"\"\"\n",
      "    picard \\\\\n",
      "        -Xmx${avail_mem}g \\\\\n",
      "        MarkDuplicates \\\\\n",
      "        $args \\\\\n",
      "        I=$bam \\\\\n",
      "        O=${prefix}.bam \\\\\n",
      "        M=${prefix}.MarkDuplicates.metrics.txt\n",
      "\n",
      "    cat <<-END_VERSIONS > versions.yml\n",
      "    \"${task.process}\":\n",
      "        picard: \\$(echo \\$(picard MarkDuplicates --version 2>&1) | grep -o 'Version:.*' | cut -f2- -d:)\n",
      "    END_VERSIONS\n",
      "    \"\"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "display_codes(same_tools_nf[3][654],nf_proc, mat_nf_lev,mat_nf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEVENSHTEIN : 1.0 \n",
      "NGRAM : 1.0 \n",
      "\n",
      "PROCESS 1\n",
      "\n",
      "process multiqc {\n",
      "    publishDir \"${params.outdir}/MultiQC\", mode: 'copy'\n",
      "\n",
      "    input:\n",
      "    file multiqc_config from ch_multiqc_config\n",
      "                                                                                  \n",
      "    file ('fastqc/*') from fastqc_results.collect().ifEmpty([])\n",
      "    file ('software_versions/*') from software_versions_yaml.collect()\n",
      "    file workflow_summary from create_workflow_summary(summary)\n",
      "\n",
      "    output:\n",
      "    file \"*multiqc_report.html\" into multiqc_report\n",
      "    file \"*_data\"\n",
      "    file \"multiqc_plots\"\n",
      "\n",
      "    script:\n",
      "    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n",
      "    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n",
      "                                                                                       \n",
      "    \"\"\"\n",
      "    multiqc -f $rtitle $rfilename --config $multiqc_config .\n",
      "    \"\"\"\n",
      "}\n",
      "PROCESS 2\n",
      "\n",
      "process qual_by_depth {\n",
      "\n",
      "      publishDir \"${output}/vcfs\", mode: 'copy', pattern: '*_filter.bcf'\n",
      "\n",
      "      input:\n",
      "          tuple val(id), file(og_vcf) from vcf2\n",
      "          tuple val(id), file(filt_vcf) from min_depth_filter\n",
      "\n",
      "      output:\n",
      "          tuple val(id), file(\"${id}_4_filter.bcf\") into qual_depth_filter\n",
      "\n",
      "      when:\n",
      "          params.vcf\n",
      "\n",
      "      \"\"\"\n",
      "          AVG_DP=`bcftools view -H ${og_vcf} | cut -f8 | grep -oe \"DP=[0-9]*\" | sed -s 's/DP=//g' | gawk '{ sum += \\$1; n++ } END { if (n > 0) print sum / n; }'`\n",
      "          DP_THRESH=`echo \"\\$AVG_DP + 4 * (sqrt(\\$AVG_DP))\" | bc`\n",
      "\n",
      "          bcftools filter --threads 8 -e \"QUAL < \\$DP_THRESH * 2 && INFO/DP > \\$DP_THRESH\" -Ob -o ${id}_4_filter.bcf ${filt_vcf}\n",
      "          bcftools index ${id}_4_filter.bcf\n",
      "      \"\"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "display_codes(diff_tools_nf[3][787],nf_proc, mat_nf_lev,mat_nf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find pairs same and diff lenghts\n",
    "sizes_nf = [len(el[\"code\"]) for el in nf_proc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_diff_sizes(sizes_nf, pairs0):\n",
    "    same_size = []\n",
    "    diff_size = []\n",
    "    small_size = []\n",
    "    large_size = []\n",
    "    for pair in pairs0:\n",
    "        if(abs(sizes_nf[pair[0]]-sizes_nf[pair[1]])<300):\n",
    "            same_size.append(pair)\n",
    "            if(sizes_nf[pair[0]]<=500 or sizes_nf[pair[1]]<=500):\n",
    "                small_size.append(pair)\n",
    "            elif(sizes_nf[pair[0]]>=1500 or sizes_nf[pair[1]]>=1500):\n",
    "                large_size.append(pair)\n",
    "        else:\n",
    "            diff_size.append(pair)\n",
    "    return same_size,diff_size,small_size,large_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_sizes_nf=[]\n",
    "diff_sizes_nf=[]\n",
    "small_sizes_nf=[]\n",
    "large_sizes_nf=[]\n",
    "sizes_nf = [len(el[\"code\"]) for el in nf_proc]\n",
    "for i in range(0,4):\n",
    "    same_new,diff_new,small_new,large_new=find_diff_sizes(sizes_nf,paires_lev_nf[i])\n",
    "    same_sizes_nf.append(same_new)\n",
    "    diff_sizes_nf.append(diff_new)\n",
    "    small_sizes_nf.append(small_new)\n",
    "    large_sizes_nf.append(large_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3105\n",
      "4710\n",
      "711\n",
      "209\n",
      "1\n",
      "1842\n",
      "2809\n",
      "731\n",
      "35\n",
      "2\n",
      "1843\n",
      "2537\n",
      "669\n",
      "44\n",
      "3\n",
      "3706\n",
      "6666\n",
      "1199\n",
      "435\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,4):\n",
    "    print(i)\n",
    "    print(len(same_sizes_nf[i]))\n",
    "    print(len(diff_sizes_nf[i]))\n",
    "    print(len(small_sizes_nf[i]))\n",
    "    print(len(large_sizes_nf[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEVENSHTEIN : 0.9940872135994088 \n",
      "NGRAM : 0.978054133138259 \n",
      "\n",
      "PROCESS 1\n",
      "process SAMTOOLS_STATS {\n",
      "    tag \"$meta.id\"\n",
      "    label 'process_low'\n",
      "\n",
      "    conda (params.enable_conda ? \"bioconda::samtools=1.15.1\" : null)\n",
      "    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n",
      "        'https://depot.galaxyproject.org/singularity/samtools:1.15.1--h1170115_0' :\n",
      "        'quay.io/biocontainers/samtools:1.15.1--h1170115_0' }\"\n",
      "\n",
      "    input:\n",
      "    tuple val(meta), path(input), path(input_index)\n",
      "    path fasta\n",
      "\n",
      "    output:\n",
      "    tuple val(meta), path(\"*.stats\"), emit: stats\n",
      "    path  \"versions.yml\"            , emit: versions\n",
      "\n",
      "    when:\n",
      "    task.ext.when == null || task.ext.when\n",
      "\n",
      "    script:\n",
      "    def args = task.ext.args ?: ''\n",
      "    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n",
      "    \"\"\"\n",
      "    samtools \\\\\n",
      "        stats \\\\\n",
      "        --threads ${task.cpus-1} \\\\\n",
      "        ${reference} \\\\\n",
      "        ${input} \\\\\n",
      "        > ${input}.stats\n",
      "\n",
      "    cat <<-END_VERSIONS > versions.yml\n",
      "    \"${task.process}\":\n",
      "        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n",
      "    END_VERSIONS\n",
      "    \"\"\"\n",
      "\n",
      "    stub:\n",
      "    def prefix = task.ext.prefix ?: \"${meta.id}\"\n",
      "    \"\"\"\n",
      "    touch ${input}.stats\n",
      "\n",
      "    cat <<-END_VERSIONS > versions.yml\n",
      "    \"${task.process}\":\n",
      "        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n",
      "    END_VERSIONS\n",
      "    \"\"\"\n",
      "}\n",
      "PROCESS 2\n",
      "\n",
      "process BuildCache_snpEff {\n",
      "  tag {snpeff_db}\n",
      "\n",
      "  publishDir params.snpeff_cache, mode: params.publish_dir_mode\n",
      "\n",
      "  input:\n",
      "    val snpeff_db from ch_snpeff_db\n",
      "\n",
      "  output:\n",
      "    file(\"*\") into snpeff_cache_out\n",
      "\n",
      "  when: params.snpeff_cache\n",
      "\n",
      "  script:\n",
      "  \"\"\"\n",
      "  snpEff download -v ${snpeff_db} -dataDir \\${PWD}\n",
      "  \"\"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "display_codes(diff_sizes_nf[3][6000],nf_proc, mat_nf_lev,mat_nf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEVENSHTEIN : 0.9693200663349917 \n",
      "NGRAM : 0.9526143790849673 \n",
      "\n",
      "PROCESS 1\n",
      "\n",
      "process SAMTOOLS_VIEW {\n",
      "    tag \"$meta.id\"\n",
      "    label 'process_medium'\n",
      "    publishDir \"${params.outdir}\",\n",
      "        mode: params.publish_dir_mode,\n",
      "        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n",
      "\n",
      "    conda (params.enable_conda ? 'bioconda::samtools=1.13' : null)\n",
      "    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n",
      "        container \"https://depot.galaxyproject.org/singularity/samtools:1.13--h8c37831_0\"\n",
      "    } else {\n",
      "        container \"quay.io/biocontainers/samtools:1.13--h8c37831_0\"\n",
      "    }\n",
      "\n",
      "    input:\n",
      "    tuple val(meta), path(bam)\n",
      "\n",
      "    output:\n",
      "    tuple val(meta), path(\"*.bam\"), emit: bam\n",
      "    path  \"versions.yml\"          , emit: versions\n",
      "\n",
      "    script:\n",
      "    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n",
      "    \"\"\"\n",
      "    samtools view $options.args $bam > ${prefix}.bam\n",
      "    cat <<-END_VERSIONS > versions.yml\n",
      "    ${getProcessName(task.process)}:\n",
      "        ${getSoftwareName(task.process)}: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n",
      "    END_VERSIONS\n",
      "    \"\"\"\n",
      "}\n",
      "PROCESS 2\n",
      "\n",
      "process multiqc {\n",
      "    publishDir \"${params.outdir}/MultiQC\", mode: params.publish_dir_mode\n",
      "\n",
      "    input:\n",
      "    file (multiqc_config) from ch_multiqc_config\n",
      "    file (mqc_custom_config) from ch_multiqc_custom_config.collect().ifEmpty([])\n",
      "                                                                                  \n",
      "    file ('fastqc/*') from ch_fastqc_results.collect().ifEmpty([])\n",
      "    file ('software_versions/*') from ch_software_versions_yaml.collect()\n",
      "    file workflow_summary from ch_workflow_summary.collectFile(name: \"workflow_summary_mqc.yaml\")\n",
      "\n",
      "    output:\n",
      "    file \"*multiqc_report.html\" into ch_multiqc_report\n",
      "    file \"*_data\"\n",
      "    file \"multiqc_plots\"\n",
      "\n",
      "    script:\n",
      "    rtitle = custom_runName ? \"--title \\\"$custom_runName\\\"\" : ''\n",
      "    rfilename = custom_runName ? \"--filename \" + custom_runName.replaceAll('\\\\W','_').replaceAll('_+','_') + \"_multiqc_report\" : ''\n",
      "    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n",
      "                                                                                       \n",
      "    \"\"\"\n",
      "    multiqc -f $rtitle $rfilename $custom_config_file .\n",
      "    \"\"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "display_codes(same_sizes_nf[3][30],nf_proc, mat_nf_lev,mat_nf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEVENSHTEIN : 0.9749478079331941 \n",
      "NGRAM : 0.9350912778904665 \n",
      "\n",
      "PROCESS 1\n",
      "\n",
      "process fastqc {\n",
      "    tag \"$name\"\n",
      "    label 'process_medium'\n",
      "    publishDir \"${params.outdir}/fastqc\", mode: params.publish_dir_mode,\n",
      "        saveAs: { filename ->\n",
      "                      filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"\n",
      "                }\n",
      "\n",
      "    input:\n",
      "    set val(name), file(reads) from ch_read_files_fastqc\n",
      "\n",
      "    output:\n",
      "    file \"*_fastqc.{zip,html}\" into ch_fastqc_results\n",
      "\n",
      "    script:\n",
      "    \"\"\"\n",
      "    fastqc --quiet --threads $task.cpus $reads\n",
      "    \"\"\"\n",
      "}\n",
      "PROCESS 2\n",
      "\n",
      "process alignDam {\n",
      "    module 'bwa/bwa-0.7.17:samtools/samtools-1.9'\n",
      "    publishDir 'unclassified'\n",
      "    cpus 14\n",
      "    memory '125 GB'\n",
      "\n",
      "    input:\n",
      "    file idx from damIndex\n",
      "    set( val (id), file (fastqs) ) from hicReadsDam\n",
      "\n",
      "    output:\n",
      "    set (val(id), file(\"dam.${id}.unclassified.bam\")) into damBamsUnclassified\n",
      "\n",
      "    \"\"\"\n",
      "    bwa mem -t ${task.cpus} dam.fa ${fastqs} | samtools view -bh - \\\n",
      "        | samtools sort -n - > dam.${id}.unclassified.bam\n",
      "    \"\"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "display_codes(small_sizes_nf[3][67],nf_proc, mat_nf_lev,mat_nf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#find pairs small and large\n",
    "fig = plt.hist(sizes_nf,range = (0, 15000), bins = 50)\n",
    "#sizes = small <500, large >1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is the similarity correctly detected ?\n",
    "\n",
    "Now we want to look at the scores of a few processes we know beforehand are similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
